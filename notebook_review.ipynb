{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle COVID-19 and the Wikidata knowledge graph\n",
    "\n",
    "We are all trying to figure out how to help us fight this pandemic. It is amazing the amount of ashtounding works being done here. \n",
    "\n",
    "\n",
    "Well, how rude of me, let's start with a proper introduction. \n",
    "\n",
    "\n",
    "Hello, I'm Tiago, a computational biology student in SÃ£o Paulo, Brazil. I'm not a virology expert, neither a NLP mage. So I was wondering: how can I actually make a contribution to this effort?\n",
    "\n",
    "\n",
    "I gave it some thought and landed on the following conclusions:\n",
    "\n",
    "### People are amazing!\n",
    "The work done by other kagglers in this effort is just is just beautiful. So, the first step of my contribution will be to *review as many COVID NLP notebooks in Kaggle as I possibly can*. Even if it is a quick review. \n",
    "\n",
    "### Everyone knows something that can help; so everyone  can contribute!\n",
    "Tasks are so diverse and require so many complementary skills that anyone can help. I could try and squeeze a life of dealing with NLP in a couple months of pandemic, but that is likely neither effective or very useful. \n",
    "There is one thing, though, that I am very familizarized with, and that is the open knowledge graph at [https://www.wikidata.org/wiki/Wikidata:Main_Page Wikidata]. What I can do is to get statements from all these amazing analysis effort and convert them  into standardized, machine-readable, referenced public domain statements. \n",
    "\n",
    "For those of you that know Wikidata, join us at our COVID-19 WikiProject! Every person counts !\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this notebook is structured?\n",
    "\n",
    "\n",
    "### Part zero: An explanation of what Wikidata is and what we can do once the statements are available in an open, interlinked data format.\n",
    "\n",
    "\n",
    "### First part: Review of current notebooks on Kaggle. \n",
    "* Goals:\n",
    "* For every work, I will write a \"twitter summary\", explaining in a couple words what was done. \n",
    "* Every work will be anotated in the following terms:\n",
    "* Which NLP tools were used?\n",
    "* Did the work output referenced statements ? \n",
    "\n",
    "### Second part: Conversion of statements into Wikidata format. \n",
    "* Part 2.0 -> A quick intro to Wikidata.\n",
    "* Goals:\n",
    "* Manually convert the statements that originated from the amazing work by other kagglers into Wikidata format. \n",
    "* Figure out ways to automatically extract relationships between terms from these datasets.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Review of Notebooks\n",
    "\n",
    "### Notebook #1 [Browsing research papers with a BM25 search engine](https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine)(v67)\n",
    "\n",
    "DwightGunning built a search engine to rank papers based on keywords related to the tasks.\n",
    "It is available in GitHub as a Python  package, cord.\n",
    "\n",
    "The user can choose subsets of papers based on dates. \n",
    "\n",
    "Output is in the format of a ranked list of papers related to the words. \n",
    "He also implements rendering of individual papers for exploration.\n",
    "\n",
    "Query mechanism based on matching tokens to the document (Abstract).\n",
    "\n",
    "Older versions of the notebook show how the search engine was built.\n",
    "\n",
    "Core tools used: Python: nltk for text processing; rank_bm25 for the searching algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #2 [CORD-19 Analysis with Sentence Embeddings](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings) (v21)\n",
    "\n",
    "David Mezzetti built a search engine to rank papers based on sentence embeddings matching a query. Furthermore, it displays the sentences from the article most likely to be of interest (i. e., highest ranking in  relation to query). \n",
    "Uses a set of heuristics to define valid sentences.\n",
    "\n",
    "A table is generated for the sentences within the article.\n",
    "Displays some EDA of the metadata related to the articles.\n",
    "\n",
    "The use of embeddings allow match of concepts instead of matching only words.\n",
    "\n",
    "It is available in GitHub as a Python  package, cord19q.\n",
    "\n",
    "David Mezzetti also rendered notebooks with highlights for each of the 10 tasks, each subdivided into specific topics, \n",
    "and defined a set of subquestions for each task.\n",
    "\n",
    "\n",
    "Core tools used: SQLite: Process metadata.csv file as a database. FastTest vectors trained on CORD dataset for embedding. BM25 index used to weight  word embeddings for reaching a sentence embedding. Python package faiss for similarity searching.  TextRank algorithm to highlight best sentences.\n",
    "\n",
    "*Note:* David's highlights are a super good resource for adding information to Wikidata.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notebook #3 [NLP Text Mining - Disease behavior](https://www.kaggle.com/cstefanache/nlp-text-mining-disease-behavior) (v16)\n",
    "\n",
    "Stefanache Cornel built a notebook on the virus behaviour, such as symptoms, incubation periods, transmission methods and so on. Built a simulator for virus propagation. \n",
    "\n",
    "He assembled terms related to COVID19 (\"incubation\", possible symtpoms, virus references, organs and connecting words as \"lower\" or \"higher\").   Coded NLP matcher functions to find matches. \n",
    "\n",
    "It is not available as a package, just many functions spread.\n",
    "\n",
    "\n",
    "Core tools used:Spacy (Python NLP suite).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notebook #4 [Anserini+BERT-SQuAD for Semantic Corpus Search](https://www.kaggle.com/dirktheeng/anserini-bert-squad-for-semantic-corpus-search) (v43)\n",
    "\n",
    "Dirk The Engineer's goal was to build a question and answering system for covid-related questions. Uses two algorithms,  Anserini and Bert-SQuAD. Anserini ranks sentences of text in relation to a question. Bert-SQuAD selects the best sentence segment to answer the question. He adapted the ideas of this paper by [Yang et al](https://arxiv.org/abs/1902.01718) to the COVID-19 dataset (actually the  lucene searchable CORD-19 database).  Google's Universal Sentence Encoder to find semantic similarities between question and answer.\n",
    "\n",
    "Defined a set of subquestions for each task.\n",
    "\n",
    "Nice visualization, highlighting the specific part of the excerpts that better answers the query.\n",
    "\n",
    "\n",
    "Core Tools used: Anserini (via Pyserini), Bert-SQuAD,  Google's Universal Sentence Encoder, BioPython/Entrez to also query PubMed\n",
    "\n",
    "It is not available as a package.\n",
    "\n",
    "\n",
    "*Note:* Q&A could be further refined if a knowledge base (as Wikidata) was populated.  Dirk's highlights are a super good resource for adding information to Wikidata. Also, this notebook is very well organized. Code folding does wonders \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #5 [CoronaWhy.org - Team - 560+ people (join slack)](    https://www.kaggle.com/arturkiulian/coronawhy-org-team-560-people-join-slack) (v44)\n",
    "\n",
    "\n",
    "Artur Kiulian, Platon Mysnyk. Ivan Didur, AntonPolishko and others organized a notebook to join people in the kaggle community to work on ML tasks related to COVID-19. THere is a Slack group that anyone can join, where specific tasks adn goals are discussed.\n",
    "\n",
    "They use also Trello taskboards and have daily calls that tackle open problems.\n",
    "Anyone can join this effort in the website [CoronaWhy.org](https://www.coronawhy.org/).\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #6 [CORD : Tools and Knowledge graphs](https://www.kaggle.com/shahules/cord-tools-and-knowledge-graphs) (v24)\n",
    "\n",
    "Shahules786 did some exploratory analysis with the dataset, checking average title sizes and most used words.\n",
    "\n",
    "Uses  Universal Sentence encoder and DBscan with spacy sentence vectors to find titles that are best match for other titles. \n",
    "\n",
    "For each task, he/she selected manually core articles, found best matches and detected keywords usind either RAKE or pytextrank. \n",
    "\n",
    "Did some work towards building knowledge graph: detecting subject, relations and objects.\n",
    "The knowledge graphs were built with spacy for each task. Visuzalization in form of networks, but it is hard to extract meaning. It is a work in progress, though, and I imagine it will get better with time.\n",
    "\n",
    "\n",
    "Core Tools used: rake-nltk, spacy matcher, tensorflow, Universal Sentence Encoder, DBscan, pytextrank\n",
    "\n",
    "It is not available as a package, just many functions spread\n",
    "\n",
    "*Note:* Knowledge graph inference very useful for wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #7 [COVID-19 Thematic tagging with Regular Expressions](https://www.kaggle.com/ajrwhite/covid-19-thematic-tagging-with-regular-expressions/) (v56)\n",
    "\n",
    "[Andy White](https://www.kaggle.com/ajrwhite) tagged articles in the corpus using regex. His tags were like `tag_disease_covid19` or `tag_risk_smoking`. Among the  motivations, the idea was to standardize some terminology and make papers easier to find. Each paper receives multiple tags. Extensive work with many terms of interest, ranging from biomedicine to geography. \n",
    "\n",
    "As of now, he is building a tool to filter papers based on tags.\n",
    "\n",
    "Core tools used: package re for identifying regex\n",
    "\n",
    "It is not available as a package, functions are available at [this link](https://www.kaggle.com/ajrwhite/covid19-tools).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #8 [CORD-19: EDA, parse JSON and generate clean CSV](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv) (v40)\n",
    "\n",
    "[xhlulu](https://www.kaggle.com/xhlulu) processed the dataset to cleand and update the part related to biorxiv papers. Helper functions are made available. He de data about the preprints available as a .csv file.\n",
    "\n",
    "\n",
    "Core tools used: pandas and basic tools for data cleaning.\n",
    "\n",
    "Not available as a package. Functions in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #9 [Mining COVID-19 scientific papers](https://www.kaggle.com/mobassir/mining-covid-19-scientific-papers) (v40)\n",
    "\n",
    "[Mobassir](https://www.kaggle.com/mobassir) focused on specifically the risk factors of COVID-19. Uses dataset of [CORD-19: EDA, parse JSON and generate clean CSV](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv). Works towards visualizing topics,  but it is hard to extract meaning.\n",
    "\n",
    "\n",
    "Core Tools used: tensorflow, nltk, bert, gensim for topic modelling, TextRank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.0 --> Wikidata : Why it can help to accelerate information exchange and synthesis. \n",
    "\n",
    "\n",
    "Wikidata is an open database, run by the Wikimedia foundation (the same that runs Wikipedia) that anyone can edit. Even though this may lead to mistakes here and there, this format enables fast correction and update of information, what is essential when we are working with novel information arising on a daily basis. \n",
    "\n",
    "Long story short: statements on Wikidata allow application of reasoning and inference algorithms, and to integrate diverse kinds of knowledge in the same place . Just the kind of multidisciplinarity we want for *actually* understanding Covid-19.\n",
    "\n",
    "\n",
    "#### What are statements in Wikidata \n",
    "\n",
    "\n",
    "Statements, that is what we like. Statements in Wikidata are composed by three parts, forming a triple. An item, a property and a value. Let's go step by step.\n",
    "\n",
    "#### **Items**:\n",
    "\n",
    "The first of the three is called an [item](https://www.wikidata.org/wiki/Help:Items). Items can be pretty much anything, from physical entities to classes of objects. This gets way clearer by exemplification:\n",
    "\n",
    "* The city of Wuhan is an item --> [Wuhan](https://www.wikidata.org/wiki/Q11746)\n",
    "* The SARS-CoV-2 strain is an item --> [SARS-CoV-2](https://www.wikidata.org/wiki/Q82069695)\n",
    "* The coronavirus pandemic in each specific place is an item--> [COVID-19 pandemic in italy](https://www.wikidata.org/wiki/Q84104992)\n",
    "\n",
    "Pretty much everything that has a specific Wikipedia page has an Wikidata item. And there are many items that don't have a Wikipedia page, so almost everything can be represented in Wikidata. \n",
    "\n",
    "( Wikidata is an ongoing effort and I believe no one is really sure of what *cannot* be modeled there. \n",
    "\n",
    "Great, so we have items! **Anyone** can create items, which means that if something is not there *you* can create it! And from the point you create the item, anyone in the world can use it. Amazing, isn't it?\n",
    "\n",
    "That is already useful for semantically tagging sentences. For example (from Wikipedia):\n",
    "\n",
    "*SARS-CoV-2 is the cause of the ongoing pandemic of coronavirus disease 2019 (COVID-19).\n",
    "\n",
    "We have te items [SARS-CoV-2](https://www.wikidata.org/wiki/Q82069695) and the  [COVID-19 pandemic](https://www.wikidata.org/wiki/Q82069695) in Wikidata that represent some of the concepts there. \n",
    "\n",
    "Each item is described by the letter Q followed by numbers, which are given on the order items are created.\n",
    "\n",
    "But this is not  enough: we want to **link** concepts! That is when the properties come.\n",
    "\n",
    "\n",
    "#### **Properties and Values**:\n",
    "\n",
    "Properties are rigorous ways to describe items. For example, every major city has a population number from local census. That is something that we want to have on Wikidata. \n",
    "So, in Wikidata, links are made between items and specific values. [Wuhan](https://www.wikidata.org/wiki/Q11746), for example, is described by a series of statements such as:\n",
    "\n",
    "* [Capital city of](1376https://www.wikidata.org/wiki/Property:P1376) --> [Hubei](https://www.wikidata.org/wiki/Q46862)\n",
    "\n",
    "* [Population](https://www.wikidata.org/wiki/Property:P1082) --> 11 895 000\n",
    "\n",
    "* [Timezone](https://www.wikidata.org/wiki/Property:P421) --> [UTC+8](https://www.wikidata.org/wiki/Q6985)\n",
    "\n",
    "\n",
    "There are thousands of properties linking items to values.  You can also link two values together (i.e. items can also be values). This is super cool, because you end up with a interconneted network of knowledge. And this knowledge network is formal enough so machines can understand\n",
    "\n",
    "* *Wait, what do you mean by understand it?*\n",
    "\n",
    "What I mean is that all this knowledge is available and searchable in many user friendly formats. \n",
    "\n",
    "#### Using Wikidata statements\n",
    "\n",
    "There are many tools built on top of Wikidata. Many Wikipedia pages have automatic infoboxes derived from Wikidata statements, for example. \n",
    "\n",
    "The one that is of greates use for us here is the SPARQL Query service, which is a way to query the Wikidata database either from the command line (via API calls) or [directly in your browser](https://query.wikidata.org/)\n",
    "A few examples of queries related to Covid-19 on Wikidata:\n",
    "* [Image grid of individuals who have died from COVID-19](https://w.wiki/LZF)\n",
    "* [List of coronaviruses](https://w.wiki/LZG) \n",
    "\n",
    "On this query you can combine different properties and ask questions such as: \n",
    "* \"which molecules interact with the ACE2 receptor and are used as treatment for some disease\"?\n",
    "* \"which viruses are catalogued as cause of a pandemic item?\"\n",
    "\n",
    "As Wikidata has information about all kinds of stuff, queries are limited by user imagination (and database completeness, of course). That's why it is important to update Wikidata with the most accurate and complete infos as possible. \n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
