{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle COVID-19 NLP - Notebook review\n",
    "\n",
    "Hello, I'm Tiago, a computational biology student in SÃ£o Paulo, Brazil. I'm not a virology expert, neither a NLP ninja. So I was wondering: how can I actually make a contribution to this effort?\n",
    "\n",
    "\n",
    "I gave it some thought and landed on the following conclusions:\n",
    "\n",
    "### People are amazing!\n",
    "The work done by other kagglers in this effort is just is just beautiful. So, the first step of my contribution will be to *review as many COVID NLP notebooks in Kaggle as I possibly can*. Even if it is a quick review. \n",
    "\n",
    "### Everyone knows something that can help; so everyone  can contribute!\n",
    "Tasks are so diverse and require so many complementary skills that anyone can help. I could try and squeeze a life of dealing with NLP in a couple months of pandemic, but that is likely neither effective or very useful. \n",
    "There is one thing, though, that I am very familizarized with, and that is the open knowledge graph at [https://www.wikidata.org/wiki/Wikidata:Main_Page Wikidata]. What I can do is to get statements from all these amazing analysis effort and convert them  into standardized, machine-readable, referenced public domain statements. tl;dr *add a Wikidata layer to Kaggle efforts*.\n",
    "\n",
    "For those of you that know Wikidata, join us at our (COVID-19 WikiProject)[https://www.wikidata.org/wiki/Wikidata_talk:WikiProject_COVID-19! Every person counts] ! \n",
    "For those of you that do not know Wikidata yet, join us anyway!\n",
    "\n",
    "\n",
    "This notebook correspond to part 1: Review of current notebooks on Kaggle. \n",
    "\n",
    "A second notebook will be linked shortly, point towards the Wikidata-related work. \n",
    "\n",
    "### First part: Review of current notebooks on Kaggle. \n",
    " For every work, I will write a small  summary, trying to summarize explaining in a couple words what was done. \n",
    " It is a personal view of what the authors aimed at doing, the core tools that were used and what can be directly useful for the wikidata integration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Review of Notebooks\n",
    "\n",
    "### Notebook #1 [Browsing research papers with a BM25 search engine](https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine)(v67)\n",
    "\n",
    "DwightGunning built a search engine to rank papers based on keywords related to the tasks.\n",
    "It is available in GitHub as a Python  package, cord.\n",
    "\n",
    "The user can choose subsets of papers based on dates. \n",
    "\n",
    "Output is in the format of a ranked list of papers related to the words. \n",
    "He also implements rendering of individual papers for exploration.\n",
    "\n",
    "Query mechanism based on matching tokens to the document (Abstract).\n",
    "\n",
    "Older versions of the notebook show how the search engine was built.\n",
    "\n",
    "Core tools used: Python: nltk for text processing; rank_bm25 for the searching algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #2 [CORD-19 Analysis with Sentence Embeddings](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings) (v21)\n",
    "\n",
    "David Mezzetti built a search engine to rank papers based on sentence embeddings matching a query. Furthermore, it displays the sentences from the article most likely to be of interest (i. e., highest ranking in  relation to query). \n",
    "Uses a set of heuristics to define valid sentences.\n",
    "\n",
    "A table is generated for the sentences within the article.\n",
    "Displays some EDA of the metadata related to the articles.\n",
    "\n",
    "The use of embeddings allow match of concepts instead of matching only words.\n",
    "\n",
    "It is available in GitHub as a Python  package, cord19q.\n",
    "\n",
    "David Mezzetti also rendered notebooks with highlights for each of the 10 tasks, each subdivided into specific topics, \n",
    "and defined a set of subquestions for each task.\n",
    "\n",
    "\n",
    "Core tools used: SQLite: Process metadata.csv file as a database. FastTest vectors trained on CORD dataset for embedding. BM25 index used to weight  word embeddings for reaching a sentence embedding. Python package faiss for similarity searching.  TextRank algorithm to highlight best sentences.\n",
    "\n",
    "*Note:* David's highlights are a super good resource for adding information to Wikidata.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notebook #3 [NLP Text Mining - Disease behavior](https://www.kaggle.com/cstefanache/nlp-text-mining-disease-behavior) (v16)\n",
    "\n",
    "Stefanache Cornel built a notebook on the virus behaviour, such as symptoms, incubation periods, transmission methods and so on. Built a simulator for virus propagation. \n",
    "\n",
    "He assembled terms related to COVID19 (\"incubation\", possible symtpoms, virus references, organs and connecting words as \"lower\" or \"higher\").   Coded NLP matcher functions to find matches. \n",
    "\n",
    "It is not available as a package, just many functions spread.\n",
    "\n",
    "\n",
    "Core tools used:Spacy (Python NLP suite).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notebook #4 [Anserini+BERT-SQuAD for Semantic Corpus Search](https://www.kaggle.com/dirktheeng/anserini-bert-squad-for-semantic-corpus-search) (v43)\n",
    "\n",
    "Dirk The Engineer's goal was to build a question and answering system for covid-related questions. Uses two algorithms,  Anserini and Bert-SQuAD. Anserini ranks sentences of text in relation to a question. Bert-SQuAD selects the best sentence segment to answer the question. He adapted the ideas of this paper by [Yang et al](https://arxiv.org/abs/1902.01718) to the COVID-19 dataset (actually the  lucene searchable CORD-19 database).  Google's Universal Sentence Encoder to find semantic similarities between question and answer.\n",
    "\n",
    "Defined a set of subquestions for each task.\n",
    "\n",
    "Nice visualization, highlighting the specific part of the excerpts that better answers the query.\n",
    "\n",
    "\n",
    "Core Tools used: Anserini (via Pyserini), Bert-SQuAD,  Google's Universal Sentence Encoder, BioPython/Entrez to also query PubMed\n",
    "\n",
    "It is not available as a package.\n",
    "\n",
    "\n",
    "*Note:* Q&A could be further refined if a knowledge base (as Wikidata) was populated.  Dirk's highlights are a super good resource for adding information to Wikidata. Also, this notebook is very well organized. Code folding does wonders \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #5 [CoronaWhy.org - Team - 560+ people (join slack)](    https://www.kaggle.com/arturkiulian/coronawhy-org-team-560-people-join-slack) (v44)\n",
    "\n",
    "\n",
    "Artur Kiulian, Platon Mysnyk. Ivan Didur, AntonPolishko and others organized a notebook to join people in the kaggle community to work on ML tasks related to COVID-19. THere is a Slack group that anyone can join, where specific tasks adn goals are discussed.\n",
    "\n",
    "They use also Trello taskboards and have daily calls that tackle open problems.\n",
    "Anyone can join this effort in the website [CoronaWhy.org](https://www.coronawhy.org/).\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #6 [CORD : Tools and Knowledge graphs](https://www.kaggle.com/shahules/cord-tools-and-knowledge-graphs) (v24)\n",
    "\n",
    "Shahules786 did some exploratory analysis with the dataset, checking average title sizes and most used words.\n",
    "\n",
    "Uses  Universal Sentence encoder and DBscan with spacy sentence vectors to find titles that are best match for other titles. \n",
    "\n",
    "For each task, he/she selected manually core articles, found best matches and detected keywords usind either RAKE or pytextrank. \n",
    "\n",
    "Did some work towards building knowledge graph: detecting subject, relations and objects.\n",
    "The knowledge graphs were built with spacy for each task. Visuzalization in form of networks, but it is hard to extract meaning. It is a work in progress, though, and I imagine it will get better with time.\n",
    "\n",
    "\n",
    "Core Tools used: rake-nltk, spacy matcher, tensorflow, Universal Sentence Encoder, DBscan, pytextrank\n",
    "\n",
    "It is not available as a package, just many functions spread\n",
    "\n",
    "*Note:* Knowledge graph inference very useful for wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #7 [COVID-19 Thematic tagging with Regular Expressions](https://www.kaggle.com/ajrwhite/covid-19-thematic-tagging-with-regular-expressions/) (v56)\n",
    "\n",
    "[Andy White](https://www.kaggle.com/ajrwhite) tagged articles in the corpus using regex. His tags were like `tag_disease_covid19` or `tag_risk_smoking`. Among the  motivations, the idea was to standardize some terminology and make papers easier to find. Each paper receives multiple tags. Extensive work with many terms of interest, ranging from biomedicine to geography. \n",
    "\n",
    "As of now, he is building a tool to filter papers based on tags.\n",
    "\n",
    "Core tools used: package re for identifying regex\n",
    "\n",
    "It is not available as a package, functions are available at [this link](https://www.kaggle.com/ajrwhite/covid19-tools).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #8 [CORD-19: EDA, parse JSON and generate clean CSV](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv) (v40)\n",
    "\n",
    "[xhlulu](https://www.kaggle.com/xhlulu) processed the dataset to cleand and update the part related to biorxiv papers. Helper functions are made available. He de data about the preprints available as a .csv file.\n",
    "\n",
    "\n",
    "Core tools used: pandas and basic tools for data cleaning.\n",
    "\n",
    "Not available as a package. Functions in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #9 [Mining COVID-19 scientific papers](https://www.kaggle.com/mobassir/mining-covid-19-scientific-papers) (v40)\n",
    "\n",
    "[Mobassir](https://www.kaggle.com/mobassir) focused on specifically the risk factors of COVID-19. Uses dataset of [CORD-19: EDA, parse JSON and generate clean CSV](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv). Works towards visualizing topics,  but it is hard to extract meaning.\n",
    "\n",
    "\n",
    "Core Tools used: tensorflow, nltk, bert, gensim for topic modelling, TextRank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook #10 [COVID-19 Literature Clustering](https://www.kaggle.com/maksimeren/covid-19-literature-clustering) (v22)\n",
    "\n",
    "[MaksimEkin](https://www.kaggle.com/maksimeren)'s goal was to cluster articles by similarity, so people can find articles that are alike. \n",
    "\n",
    "It uses sklearn HashingVectorizer and Tf-idf to create a feature vector for each article from its digrams (ex: 'liveattenuated', 'attenuatedviruses',\n",
    " 'viruseshave'). \n",
    " \n",
    " Visualizes the papers using PCA/TSNE + seaborn. Clusters the articles using kmeans.\n",
    " \n",
    "He plays around with number of clusters and creates na interactive tsne viz.\n",
    "\n",
    "Core Tools: sklearn\n",
    "\n",
    "Not available as a package. Functions in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress\n",
    "\n",
    "To do: review more notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
